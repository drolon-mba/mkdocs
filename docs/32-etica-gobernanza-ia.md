# 32 - √âtica y Gobernanza de IA

> √âtica, sesgos, fairness, explicabilidad y gobernanza en sistemas de IA.

[üè† Volver al √≠ndice](./00-indice.md)

---

## üìã √çndice R√°pido

- [‚öñÔ∏è Introducci√≥n](#introduccion)
- [üéØ Bias en Machine Learning](#bias-en-machine-learning)
- [üëÆ‚Äç‚ôÇÔ∏è Gesti√≥n de Riesgo y Auditor√≠a Humana](#gestion-de-riesgo-y-auditoria-humana)
- [üìä Fairness Metrics](#fairness-metrics)
- [üîç Explicabilidad (XAI)](#explicabilidad-xai)
- [üîê Privacy](#privacy)
- [üìã Gobernanza](#gobernanza)
- [üìã Artefactos](#artefactos)

---

## ‚öñÔ∏è Introducci√≥n

**Qu√©:** Principios y pr√°cticas para construir sistemas de IA √©ticos, justos y transparentes.

**Por qu√©:** IA sin √©tica puede amplificar sesgos, discriminar y causar da√±o a escala. Regulaciones (GDPR, AI Act) exigen responsabilidad.

**C√≥mo:** Detectar y mitigar bias, medir fairness, explicar decisiones, proteger privacy, documentar modelos.

### Principios Fundamentales

| Principio | Explicaci√≥n |
|:----------|:------------|
| **Fairness** | El modelo no debe discriminar por raza, g√©nero, edad, etc. |
| **Transparency** | Las decisiones del modelo deben ser explicables |
| **Privacy** | Proteger datos personales y sensibles |
| **Accountability** | Responsabilidad clara por decisiones del modelo |
| **Safety** | El modelo no debe causar da√±o |

---

## üéØ Bias en Machine Learning

### Tipos de Bias

| Tipo | Descripci√≥n | Ejemplo |
|:-----|:------------|:--------|
| **Historical Bias** | Bias en datos hist√≥ricos refleja discriminaci√≥n pasada | Dataset de contrataciones con m√°s hombres en tech ‚Üí modelo aprende a preferir hombres |
| **Representation Bias** | Grupos subrepresentados en datos de entrenamiento | Dataset de reconocimiento facial con 90% personas blancas ‚Üí peor performance en personas de color |
| **Measurement Bias** | Features usadas como proxies para atributos protegidos | Usar c√≥digo postal como proxy para raza/ingresos |
| **Aggregation Bias** | Modelo √∫nico para grupos diversos con necesidades diferentes | Modelo de diagn√≥stico m√©dico entrenado solo en adultos, usado en ni√±os |
| **Evaluation Bias** | Benchmark no representa poblaci√≥n real | Evaluar modelo de NLP solo en ingl√©s formal, usar en slang |

 ---

## üëÆ‚Äç‚ôÇÔ∏è Gesti√≥n de Riesgo y Auditor√≠a Humana

### Clasificaci√≥n de Riesgo (EU AI Act)

 | Nivel | Descripci√≥n | Ejemplos | Requisito de Auditor√≠a |
 |:------|:------------|:---------|:-----------------------|
 | **Riesgo Inaceptable** | Amenaza a seguridad o derechos | Social scoring, manipulaci√≥n subliminal | üõë **Prohibido** |
 | **Alto Riesgo** | Infraestructura cr√≠tica, empleo, servicios esenciales | Auth, Crypto, Hiring, Cr√©dito | üëÆ **Auditor√≠a Humana Obligatoria** |
 | **Riesgo Limitado** | Chatbots, deepfakes | Customer service, generaci√≥n de contenido | ‚ö†Ô∏è **Transparencia** (avisar que es IA) |
 | **Riesgo M√≠nimo** | Filtros de spam, juegos | Videojuegos, filtros de correo | ‚úÖ **Libre** (sin requisitos extra) |

### Protocolo de Auditor√≠a Humana (Human-in-the-loop)

 Para componentes de **Alto Riesgo** (ej: Autenticaci√≥n, Cifrado, Pagos):

 1. **Prohibici√≥n de Commit Directo:** IA no puede commitear a `main` sin review humano.
 2. **Review de Seguridad:** Experto humano debe validar l√≠nea por l√≠nea (no "LGTM" r√°pido).
 3. **Sandboxing:** C√≥digo generado corre en entorno aislado primero.
 4. **Firma Digital:** El humano firma que revis√≥ y asume responsabilidad (Accountability).

 > [!IMPORTANT]
 > **Nunca delegues decisiones arquitect√≥nicas cr√≠ticas o de seguridad a la IA.** La IA es un copiloto, el humano es el capit√°n.

---

### Detecci√≥n de Bias

#### 1. An√°lisis de Datos

```python
import pandas as pd

# Verificar balance de clases por grupo protegido
df.groupby(['protected_attribute', 'target']).size()

# Ejemplo: Dataset de cr√©dito
#                    approved
# gender  age_group          
# female  <30           1200
#         30-50         3400
#         >50           2100
# male    <30           1800
#         30-50         4200
#         >50           2500

# ‚ö†Ô∏è Warning: Mujeres <30 subrepresentadas
```

---

#### 2. Fairness Metrics (ver secci√≥n siguiente)

---

#### 3. Feature Importance por Grupo

```python
from sklearn.inspection import permutation_importance

# Calcular feature importance por grupo
for group in df['protected_attribute'].unique():
    group_data = df[df['protected_attribute'] == group]
    importance = permutation_importance(model, group_data[features], group_data['target'])
    print(f"Group {group}: {importance.importances_mean}")

# ‚ö†Ô∏è Si features diferentes son importantes para diferentes grupos ‚Üí posible bias
```

---

### Mitigaci√≥n de Bias

| T√©cnica | Cu√°ndo Usar | Ejemplo |
|:--------|:------------|:--------|
| **Pre-processing** | Antes de entrenar modelo | Rebalancear dataset (oversampling, undersampling, SMOTE) |
| **In-processing** | Durante entrenamiento | Agregar fairness constraints (adversarial debiasing, reweighting) |
| **Post-processing** | Despu√©s de entrenar modelo | Ajustar thresholds por grupo para igualar m√©tricas de fairness |

---

#### Ejemplo: Rebalanceo con SMOTE

```python
from imblearn.over_sampling import SMOTE

# Rebalancear dataset por grupo protegido
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Verificar balance
pd.Series(y_resampled).value_counts()
```

---

#### Ejemplo: Fairness Constraints

```python
from aif360.algorithms.inprocessing import AdversarialDebiasing

# Entrenar modelo con fairness constraint
debiased_model = AdversarialDebiasing(
    privileged_groups=[{'gender': 1}],  # Hombres
    unprivileged_groups=[{'gender': 0}],  # Mujeres
    scope_name='debiased_classifier',
    debias=True
)
debiased_model.fit(dataset)
```

---

## üìä Fairness Metrics

### Demographic Parity

**Definici√≥n:** Probabilidad de outcome positivo debe ser igual para todos los grupos.

**F√≥rmula:**

```text
P(Y_pred=1 | A=0) = P(Y_pred=1 | A=1)
```

**Ejemplo:**

```python
# Calcular demographic parity
def demographic_parity(y_pred, protected_attr):
    groups = protected_attr.unique()
    rates = {}
    for group in groups:
        mask = protected_attr == group
        rates[group] = y_pred[mask].mean()
    return rates

# Ejemplo: Aprobaci√≥n de cr√©dito
# Group 0 (mujeres): 0.65
# Group 1 (hombres): 0.70
# Diferencia: 0.05 ‚Üí ‚ö†Ô∏è Posible bias
```

**Cu√°ndo usar:**

- Cuando queremos igualdad de oportunidades (ej: contrataci√≥n, admisiones)

**Limitaci√≥n:**

- Puede forzar igualdad de outcomes incluso si hay diferencias leg√≠timas en inputs

---

### Equalized Odds

**Definici√≥n:** True positive rate y false positive rate deben ser iguales para todos los grupos.

**F√≥rmula:**

```text
P(Y_pred=1 | Y_true=1, A=0) = P(Y_pred=1 | Y_true=1, A=1)  # TPR
P(Y_pred=1 | Y_true=0, A=0) = P(Y_pred=1 | Y_true=0, A=1)  # FPR
```

**Ejemplo:**

```python
from sklearn.metrics import confusion_matrix

def equalized_odds(y_true, y_pred, protected_attr):
    groups = protected_attr.unique()
    metrics = {}
    for group in groups:
        mask = protected_attr == group
        tn, fp, fn, tp = confusion_matrix(y_true[mask], y_pred[mask]).ravel()
        tpr = tp / (tp + fn)
        fpr = fp / (fp + tn)
        metrics[group] = {'TPR': tpr, 'FPR': fpr}
    return metrics

# Ejemplo: Predicci√≥n de reincidencia
# Group 0: TPR=0.75, FPR=0.20
# Group 1: TPR=0.80, FPR=0.25
# ‚ö†Ô∏è Diferencias en TPR y FPR ‚Üí posible bias
```

**Cu√°ndo usar:**

- Cuando queremos igualdad en accuracy para todos los grupos (ej: diagn√≥stico m√©dico, predicci√≥n de riesgo)

---

### Calibration

**Definici√≥n:** Entre individuos con misma probabilidad predicha, la fracci√≥n de outcomes positivos debe ser igual para todos los grupos.

**F√≥rmula:**

```text
P(Y_true=1 | Y_pred=p, A=0) = P(Y_true=1 | Y_pred=p, A=1)
```

**Cu√°ndo usar:**

- Cuando las probabilidades predichas se usan para tomar decisiones (ej: scoring de cr√©dito)

---

### Trade-offs entre Fairness Metrics

> [!WARNING]
> Es matem√°ticamente imposible satisfacer todas las fairness metrics simult√°neamente (excepto en casos triviales).

**Ejemplo:**

- Demographic parity vs Equalized odds: Si hay diferencias reales en base rates entre grupos, no se pueden satisfacer ambas.

**Soluci√≥n:**

- Elegir la m√©trica m√°s apropiada para el contexto de negocio y regulaciones aplicables.

---

## üîç Explicabilidad (XAI)

### ¬øPor Qu√© Explicabilidad?

| Raz√≥n | Ejemplo |
|:------|:--------|
| **Regulaciones** | GDPR "derecho a explicaci√≥n" |
| **Trust** | Usuarios conf√≠an m√°s en decisiones explicables |
| **Debugging** | Detectar errores en modelo |
| **Fairness** | Verificar que modelo no usa features discriminatorias |

---

### SHAP (SHapley Additive exPlanations)

**Qu√© hace:** Explica contribuci√≥n de cada feature a la predicci√≥n.

**Ejemplo:**

```python
import shap

# Entrenar modelo
model = XGBClassifier()
model.fit(X_train, y_train)

# Calcular SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Visualizar
shap.summary_plot(shap_values, X_test)
```

**Interpretaci√≥n:**

- Features con SHAP values positivos ‚Üí aumentan probabilidad de clase positiva
- Features con SHAP values negativos ‚Üí disminuyen probabilidad

---

### LIME (Local Interpretable Model-agnostic Explanations)

**Qu√© hace:** Explica predicciones individuales aproximando modelo complejo con modelo simple local.

**Ejemplo:**

```python
from lime.lime_tabular import LimeTabularExplainer

# Crear explainer
explainer = LimeTabularExplainer(
    X_train.values,
    feature_names=X_train.columns,
    class_names=['rejected', 'approved'],
    mode='classification'
)

# Explicar predicci√≥n individual
exp = explainer.explain_instance(
    X_test.iloc[0].values,
    model.predict_proba
)
exp.show_in_notebook()
```

---

### Feature Importance

**Qu√© hace:** Ranking de features por importancia global.

**Ejemplo:**

```python
from sklearn.inspection import permutation_importance

# Calcular permutation importance
result = permutation_importance(model, X_test, y_test, n_repeats=10)

# Visualizar
import matplotlib.pyplot as plt
sorted_idx = result.importances_mean.argsort()
plt.barh(X_test.columns[sorted_idx], result.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")
```

---

## üîê Privacy

### Differential Privacy

**Qu√© es:** Garant√≠a matem√°tica de que agregar/remover un individuo del dataset no cambia significativamente el output.

**Ejemplo:**

```python
from diffprivlib.models import LogisticRegression

# Entrenar modelo con differential privacy
model = LogisticRegression(epsilon=1.0)  # Privacy budget
model.fit(X_train, y_train)

# Modelo entrenado sin ver datos individuales directamente
```

**Trade-off:**

- ‚úÖ Privacy garantizada matem√°ticamente
- ‚ùå Accuracy reducida (m√°s privacy ‚Üí menos accuracy)

---

### Federated Learning

**Qu√© es:** Entrenar modelo sin centralizar datos (modelo viaja a los datos, no al rev√©s).

**Ejemplo:**

```python
# Pseudoc√≥digo
# Servidor central
global_model = initialize_model()

for round in range(num_rounds):
    # Enviar modelo a clientes
    for client in clients:
        local_model = client.train(global_model, local_data)
        send_to_server(local_model.weights)
    
    # Agregar pesos locales
    global_model.weights = average(all_local_weights)
```

**Ventajas:**

- ‚úÖ Datos nunca salen del dispositivo del usuario
- ‚úÖ Compliance con GDPR, HIPAA

**Desaf√≠os:**

- ‚ùå Heterogeneidad de datos entre clientes
- ‚ùå Comunicaci√≥n costosa

---

### Data Anonymization

**T√©cnicas:**

| T√©cnica | Descripci√≥n | Ejemplo |
|:--------|:------------|:--------|
| **K-anonymity** | Cada registro es indistinguible de al menos k-1 otros | Generalizar edad (25 ‚Üí 20-30) |
| **L-diversity** | Cada grupo de k registros tiene al menos L valores distintos en atributos sensibles | Grupo de 5 personas con 3 diagn√≥sticos diferentes |
| **T-closeness** | Distribuci√≥n de atributos sensibles en cada grupo es similar a distribuci√≥n global | Distribuci√≥n de salarios en grupo similar a distribuci√≥n general |

---

## üìã Gobernanza

### Model Cards

**Qu√© es:** Documentaci√≥n estandarizada de modelos ML.

**Contenido:**

- **Model Details**: Tipo, arquitectura, versi√≥n
- **Intended Use**: Para qu√© fue dise√±ado, limitaciones
- **Factors**: Variables que afectan performance (demograf√≠a, contexto)
- **Metrics**: Accuracy, fairness metrics
- **Training Data**: Descripci√≥n, fuentes, bias conocidos
- **Ethical Considerations**: Riesgos, mitigaciones

**Ejemplo:**

```markdown
# Model Card: Credit Approval Model

## Model Details
- Type: XGBoost Classifier
- Version: 2.1.0
- Trained: 2024-01-15

## Intended Use
- Primary use: Approve/reject credit applications
- Out-of-scope: Not for use in countries with different regulations

## Factors
- Evaluated across gender, age, income brackets
- Performance varies by age group (lower accuracy for <25)

## Metrics
- Overall Accuracy: 0.87
- Demographic Parity Difference: 0.03
- Equalized Odds Difference: 0.05

## Training Data
- Source: Internal credit applications 2020-2023
- Size: 500K applications
- Known bias: Underrepresentation of applicants <25

## Ethical Considerations
- Risk: May perpetuate historical bias in credit approval
- Mitigation: Applied SMOTE rebalancing, fairness constraints
```

---

### Datasheets for Datasets

**Qu√© es:** Documentaci√≥n estandarizada de datasets.

**Contenido:**

- **Motivation**: Por qu√© se cre√≥ el dataset
- **Composition**: Qu√© contiene, cu√°ntos ejemplos, missing data
- **Collection Process**: C√≥mo se recolectaron los datos
- **Preprocessing**: Qu√© transformaciones se aplicaron
- **Uses**: Para qu√© se puede/no se puede usar
- **Distribution**: C√≥mo se distribuye, licencia
- **Maintenance**: Qui√©n mantiene, c√≥mo reportar errores

---

### AI Ethics Boards

**Qu√© es:** Comit√© que revisa proyectos de IA para identificar riesgos √©ticos.

**Composici√≥n:**

- T√©cnicos (ML engineers, data scientists)
- Expertos en √©tica
- Legal/compliance
- Representantes de grupos afectados

**Proceso:**

1. **Submission**: Equipo env√≠a propuesta de proyecto de IA
2. **Review**: Ethics board eval√∫a riesgos (bias, privacy, safety)
3. **Decision**: Aprobar, aprobar con condiciones, rechazar
4. **Monitoring**: Seguimiento post-deployment

---

## üìã Artefactos

### Bias Detection Checklist

- [ ] **Representatividad**: ¬øTodos los grupos est√°n representados proporcionalmente?
- [ ] **Balance**: ¬øLas clases est√°n balanceadas por grupo protegido?
- [ ] **Proxies**: ¬øHay features que son proxies para atributos protegidos? (ej: c√≥digo postal ‚Üí raza)
- [ ] **Historical bias**: ¬øLos datos hist√≥ricos reflejan discriminaci√≥n pasada?
- [ ] **Measurement bias**: ¬øLas features se miden de forma consistente para todos los grupos?
- [ ] **Evaluation**: ¬øEl benchmark representa la poblaci√≥n real?

---

### Model Card Template

```markdown
# Model Card: [Model Name]

## Model Details
- **Type**: [Classification/Regression/etc]
- **Architecture**: [XGBoost/Neural Network/etc]
- **Version**: [1.0.0]
- **Trained**: [YYYY-MM-DD]
- **Owner**: [Team/Person]

## Intended Use
- **Primary use**: [Description]
- **Out-of-scope uses**: [What NOT to use for]
- **Users**: [Who should use this model]

## Factors
- **Groups**: [Demographic groups evaluated]
- **Instrumentation**: [How data was collected]
- **Environment**: [Deployment environment]

## Metrics
- **Overall Accuracy**: [0.XX]
- **Precision/Recall**: [0.XX / 0.XX]
- **Fairness Metrics**:
  - Demographic Parity Difference: [0.XX]
  - Equalized Odds Difference: [0.XX]

## Training Data
- **Source**: [Where data came from]
- **Size**: [N examples]
- **Timeframe**: [YYYY-MM to YYYY-MM]
- **Known Bias**: [Describe bias]

## Ethical Considerations
- **Risks**: [Potential harms]
- **Mitigations**: [What was done to reduce risks]
- **Limitations**: [Known limitations]

## Caveats and Recommendations
- [Caveat 1]
- [Caveat 2]
```

---

### Ethical Review Template

```markdown
# Ethical Review: [Project Name]

## Project Overview
- **Description**: [What the AI system does]
- **Stakeholders**: [Who is affected]
- **Impact**: [Expected impact]

## Risk Assessment

### Bias Risk
- **Risk Level**: [Low/Medium/High]
- **Description**: [Potential bias]
- **Mitigation**: [How to address]

### Privacy Risk
- **Risk Level**: [Low/Medium/High]
- **Description**: [Privacy concerns]
- **Mitigation**: [How to protect privacy]

### Safety Risk
- **Risk Level**: [Low/Medium/High]
- **Description**: [Potential harm]
- **Mitigation**: [Safety measures]

### Fairness Risk
- **Risk Level**: [Low/Medium/High]
- **Description**: [Fairness concerns]
- **Mitigation**: [Fairness interventions]

## Decision
- [ ] Approved
- [ ] Approved with conditions: [List conditions]
- [ ] Rejected: [Reason]

## Monitoring Plan
- **Metrics to track**: [Fairness metrics, performance metrics]
- **Frequency**: [Weekly/Monthly]
- **Responsible**: [Team/Person]
```

---

## üìö Recursos

- [AI Fairness 360 (IBM)](https://aif360.mybluemix.net/)
- [Fairlearn (Microsoft)](https://fairlearn.org/)
- [Model Cards Paper (Google)](https://arxiv.org/abs/1810.03993)
- [Datasheets for Datasets (Microsoft)](https://arxiv.org/abs/1803.09010)
- [GDPR Right to Explanation](https://gdpr-info.eu/)

---

[‚¨ÖÔ∏è Anterior: Estrategia de IA](./31-estrategia-ia-automatizacion.md) | [‚¨ÜÔ∏è Volver arriba](#32-etica-y-gobernanza-de-ia) | [‚û°Ô∏è Siguiente: Comunicaci√≥n y Contenido](./33-comunicacion-contenido.md)
