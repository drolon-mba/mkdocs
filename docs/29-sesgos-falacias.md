# 29 - Sesgos Cognitivos, Falacias y Leyes a Evitar

> Trampas mentales, errores de razonamiento y leyes que impactan toma de decisiones en tecnolog√≠a, producto y negocio.

[üè† Volver al √≠ndice](./00-indice.md)

---

## üìã √çndice R√°pido

- [üß† ¬øPor qu√© importan los Sesgos?](#por-que-importan-los-sesgos)
- [üìê Leyes Fundamentales en Tecnolog√≠a](#leyes-fundamentales-en-tecnologia)
- [üéØ Sesgos de Decisi√≥n y Juicio](#sesgos-de-decision-y-juicio)
- [üìä Sesgos en Datos y An√°lisis](#sesgos-en-datos-y-analisis)
- [üë• Sesgos Sociales y de Grupo](#sesgos-sociales-y-de-grupo)
- [üîç Sesgos Perceptuales y de Evaluaci√≥n](#sesgos-perceptuales-y-de-evaluacion)
- [‚öñÔ∏è Falacias L√≥gicas Cr√≠ticas](#falacias-logicas-criticas)
- [‚ö†Ô∏è Leyes y Efectos Parad√≥jicos](#leyes-y-efectos-paradojicos)
- [üìä Sesgos en Data Science y ML](#sesgos-en-data-science-y-ml)
- [üõ°Ô∏è Estrategias de Mitigaci√≥n](#estrategias-de-mitigacion)
- [üö´ Se√±ales de Alerta: C√≥mo Reconocer Est√°s Sesgado](#senales-de-alerta-como-reconocer-estas-sesgado)
- [‚úÖ Checklist Anti-Sesgos](#checklist-anti-sesgos)
- [üéØ Template para RFCs Anti-Sesgo](#template-para-rfcs-anti-sesgo)
- [üéØ Casos Pr√°cticos en Tecnolog√≠a](#casos-practicos-en-tecnologia)
- [üìö Recursos](#recursos)
- [üí° Clave para Semi-Seniors](#clave-para-semi-seniors)
---

## üß† ¬øPor qu√© importan los Sesgos?

**What:** Atajos mentales (heur√≠sticos) que sistem√°ticamente desv√≠an el juicio de la racionalidad.

**Why:** Todos tenemos sesgos. Reconocerlos previene malas decisiones en producto, inversi√≥n, arquitectura y management.

**Who:** Product Managers, founders, investors, developers, leaders.

**When:** Dise√±o de producto, decisiones de inversi√≥n, evaluaci√≥n de performance, priorizaci√≥n.

**How:** Awareness, procesos estructurados, devil's advocate, datos sobre intuici√≥n.

**How much:** Un sesgo no detectado puede costar millones en producto fallido o inversi√≥n perdida.

---

## üìê Leyes Fundamentales en Tecnolog√≠a

| Ley | Definici√≥n | Aplicaci√≥n en Tech/Negocio | Ejemplo Real |
|:----|:-----------|:---------------------------|:-------------|
| **Ley de Murphy** | "Si algo puede salir mal, saldr√° mal" | Siempre tener plan B, runbooks, rollback plan | Deploy sin rollback ‚Üí ca√≠da total del sistema |
| **Ley de Kiddlin** | "Si escribes el problema con suficiente claridad, ya tienes la mitad de la soluci√≥n" | 5W2H, SCQA framework, documentar problemas claramente | Documentar bug con contexto completo acelera resoluci√≥n |
| **Ley de Gilbert** | "Cuando act√∫as, el √©xito no est√° garantizado. Pero cuando no act√∫as, el fracaso s√≠ lo est√°" | Bias to action, MVPs, fail fast | No lanzar MVP por miedo = oportunidad perdida |
| **Ley de Wilson** | "Si priorizas el conocimiento sobre la sabidur√≠a, el progreso te har√° m√°s est√∫pido" | Balance entre learning y wisdom, senior judgement | Adoptar frameworks sin criterio de cu√°ndo usarlos |
| **Ley de Faulkland** | "Cuando no necesites tomar una decisi√≥n, no la tomes" | Reversible decisions, defer until necessary | Decisiones de arquitectura: esperar hasta tener m√°s informaci√≥n |
| **Primera Ley de Newton** | "Un objeto en movimiento permanece en movimiento" | Momentum matters, consistency beats intensity | Daily commits consistentes > sprints intensos espor√°dicos |
| **Principio de Pareto (80/20)** | 80% resultados de 20% esfuerzo | Identificar y optimizar el 20% cr√≠tico | 20% de features generan 80% del valor |
| **Ley de Parkinson** | "El trabajo se expande para llenar el tiempo disponible" | Timeboxing, sprints cortos (1-2 semanas) | Sprint de 4 semanas ‚Üí scope creep inevitable |
| **Ley de Hick** | "Tiempo de decisi√≥n aumenta logar√≠tmicamente con opciones" | Limitar opciones en UI, simplificar decisiones | Dropdown con 50 opciones vs b√∫squeda con autocompletado |
| **Navaja de Ockham** | "La explicaci√≥n m√°s simple suele ser la correcta" | KISS principle, evitar over-engineering | Debugging: buscar causas simples primero |
| **Ley de Conway** | "Sistemas reflejan estructura de comunicaci√≥n organizacional" | Dise√±ar equipos y comunicaci√≥n antes que arquitectura | Equipos aislados ‚Üí arquitectura monol√≠tica |
| **Goodhart's Law** | "Cuando una medida se convierte en objetivo, deja de ser buena medida" | Medir m√∫ltiples dimensiones, cambiar m√©tricas | Optimizar lines of code ‚Üí c√≥digo inflado |
| **Cobra Effect** | "Soluci√≥n al problema empeora el problema" | Anticipar incentivos perversos | Pagar por bugs reportados ‚Üí crear bugs ficticios |

> üí° **Patr√≥n anti-sesgo:** Implementar "pre-mortem" antes de cada release: *"Es 3 meses despu√©s, el feature fall√≥. ¬øPor qu√©?"*

---

## üéØ Sesgos de Decisi√≥n y Juicio

| Sesgo | What | Why ocurre | When aparece | Impacto | C√≥mo Mitigar |
|:------|:-----|:-----------|:-------------|:--------|:-------------|
| **Confirmation Bias (Confirmaci√≥n)** | Buscar solo evidencia que confirme creencias pre-existentes | Cerebro busca coherencia y evita disonancia cognitiva | Evaluar nuevas tecnolog√≠as, revisar m√©tricas, code review | Ignorar se√±ales de que el producto no funciona, mantener c√≥digo legacy innecesariamente | Pre-mortem: "¬øPor qu√© podr√≠a fallar esto?", buscar evidencia contradictoria activamente |
| **Anchoring Bias (Anclaje)** | Sobre-ponderar la primera informaci√≥n recibida | La informaci√≥n inicial establece un punto de referencia mental | Estimaciones de tiempo, negociaciones salariales, planning de sprints | Estimaciones sesgadas por n√∫mero inicial, presupuestos irreales | M√∫ltiples estimaciones independientes, usar base rates hist√≥ricos |
| **Availability Bias (Disponibilidad)** | Sobrevalorar informaci√≥n f√°cilmente recordable (reciente, dram√°tica) | Memoria privilegia eventos v√≠vidos y recientes | Evaluar riesgos despu√©s de un incidente, priorizar features | Temer riesgos raros pero visibles, ignorar problemas comunes pero menos dram√°ticos | Datos estad√≠sticos en lugar de an√©cdotas, mantener registros hist√≥ricos |
| **Recency Bias (Actualidad)** | Ponderar m√°s eventos recientes | Memoria de trabajo tiene capacidad limitada | Despu√©s de un deploy fallido, evaluaci√≥n trimestral | "√öltima release fall√≥" ‚Üí evitar deploys por miedo injustificado | Analizar tendencias a largo plazo, no eventos aislados |
| **Hindsight Bias (Retrospectiva)** | "Yo lo sab√≠a desde el principio" despu√©s del hecho | Cerebro reescribe recuerdos para ser consistente | Post-mortems, revisiones de decisiones pasadas | No aprender de errores reales, sobrestimar capacidad predictiva | Documentar predicciones antes de conocer resultados, blameless postmortems |
| **Overconfidence Bias (Exceso de confianza)** | Sobreestimar habilidades y probabilidad de √©xito | Falta de feedback inmediato sobre errores de juicio | Estimaciones de proyectos, evaluaci√≥n de habilidades t√©cnicas | Estimaciones optimistas, no preparar contingencias, bugs en producci√≥n | Calibraci√≥n con track record hist√≥rico, revisiones por pares |
| **Planning Fallacy** | Subestimar tiempo/costo, sobrestimar beneficios | Enfoque en escenario ideal sin considerar imprevistos | Planning de sprints, roadmap de producto | Proyectos tarde y sobre presupuesto, equipos sobrecargados | Reference class forecasting, buffer time basado en hist√≥rico (25%) |
| **Sunk Cost Fallacy (Costo hundido)** | Continuar invirtiendo porque ya invertiste mucho | Aversi√≥n a admitir p√©rdidas, deseo de justificar decisiones pasadas | Mantener proyectos fallidos, seguir con tecnolog√≠a obsoleta | Mantener proyectos fallidos mucho tiempo, waste de recursos | Decisiones basadas en valor futuro, no pasado; kill criteria definidas |
| **Status Quo Bias** | Preferir estado actual, resistir cambio | Conservaci√≥n de energ√≠a mental, miedo a lo desconocido | Adopci√≥n de nuevas tecnolog√≠as, cambios de proceso | No adoptar mejores tecnolog√≠as/procesos, estancamiento t√©cnico | Forzar justificaci√≥n del status quo tambi√©n, experimentos controlados |
| **Loss Aversion (Aversi√≥n a p√©rdida)** | P√©rdidas duelen 2x m√°s que ganancias equivalentes | Mecanismo evolutivo de supervivencia | Decisiones de refactoring, inversi√≥n en tech debt | Evitar riesgos necesarios, no innovar, acumulaci√≥n de tech debt | Enmarcar como p√©rdida de oportunidad si no act√∫as, dividir en pasos peque√±os |
| **Regret Aversion (Aversi√≥n al arrepentimiento)** | Evitar decisiones que podr√≠an causar arrepentimiento | Miedo a consecuencias emocionales de errores | Cambios arquitect√≥nicos grandes, hiring/firing | Par√°lisis por an√°lisis, no tomar decisiones reversibles | Distinguir decisiones reversibles vs irreversibles, timeboxing de decisiones |

---

## üìä Sesgos en Datos y An√°lisis

| Sesgo | What | Why ocurre | Impacto | Ejemplo | C√≥mo Mitigar |
|:------|:-----|:-----------|:--------|:--------|:-------------|
| **Survivorship Bias (Supervivencia)** | Analizar solo lo que "sobrevivi√≥", ignorar lo que fall√≥ | Los fracasos son menos visibles y se olvidan | Conclusiones err√≥neas sobre causas de √©xito | Estudiar startups exitosas sin analizar las que fallaron | Incluir data de failures, buscar casos de estudio negativos |
| **Selection Bias (Selecci√≥n)** | Muestra no representativa de poblaci√≥n | Acceso limitado a ciertos grupos de datos | Decisiones basadas en data sesgada | User surveys solo responden los muy satisfechos/insatisfechos | Random sampling, analizar non-responders, m√∫ltiples fuentes |
| **Cherry Picking** | Seleccionar data que apoya tu argumento | Sesgo de confirmaci√≥n en an√°lisis de datos | Validar hip√≥tesis falsas | Mostrar solo trimestres de crecimiento, ocultar los malos | Pre-registrar an√°lisis, third-party review, an√°lisis ciego |
| **Sampling Bias** | Muestra no aleatoria de la poblaci√≥n | M√©todos de recolecci√≥n imperfectos | Resultados no generalizables | A/B test solo en power users, ignorando nuevos usuarios | Stratified sampling, randomizaci√≥n, tama√±o muestral adecuado |
| **Response Bias** | Respondientes difieren de no-respondientes | Caracter√≠sticas que afectan disposici√≥n a responder | Survey results sesgados | Solo usuarios muy engaged responden NPS | Follow-up con non-responders, incentivos balanceados |
| **Base Rate Neglect (Negaci√≥n ratio base)** | Ignorar probabilidades base, foco en caso espec√≠fico | Casos espec√≠ficos son m√°s memorables | Sobreestimar probabilidad de eventos raros | "Este startup es el pr√≥ximo Google" (ignorando que 90% fallan) | Siempre empezar con base rates, usar estad√≠stica bayesiana |
| **Volatility Bias** | Confundir volatilidad con riesgo | Aversi√≥n a variabilidad sin analizar direcci√≥n | Evitar inversiones vol√°tiles pero rentables | Evitar crypto por volatilidad, no por fundamentos | Separar volatilidad de riesgo direccional |
| **Evaluator Bias (Del evaluador)** | Quien eval√∫a afecta resultado | Diferentes est√°ndares entre evaluadores | Performance reviews inconsistentes | Manager laxo vs estricto | Calibraci√≥n, m√∫ltiples evaluadores |

---

## üë• Sesgos Sociales y de Grupo

| Sesgo | What | Why ocurre | Impacto | C√≥mo Mitigar |
|:------|:-----|:-----------|:--------|:-------------|
| **Authority Bias (Autoridad)** | Confiar excesivamente en "expertos" | Estructura social jer√°rquica, deferencia a experiencia | Seguir malas ideas de seniors, falta de pensamiento cr√≠tico | Question everything, "disagree and commit", meritocracia de ideas |
| **Halo Effect (Efecto halo)** | Atributo positivo influye juicio general | Generalizaci√≥n de impresiones | Sobrevalorar a persona por un logro aislado | Evaluar dimensiones independientemente, rubricas estructuradas |
| **Horn Effect (Efecto cuerno)** | Atributo negativo contamina todo | Generalizaci√≥n negativa por primera impresi√≥n | Subestimar por error pasado, oportunidades perdidas | Separar evaluaciones, fresh start mentality, focus en mejora |
| **False Consensus (Falso consenso)** | Asumir que otros piensan como t√∫ | Proyecci√≥n de propias creencias | Dise√±ar para ti, no para usuarios, features no usadas | User research, diverse team, testing con usuarios reales |
| **Similarity Bias (Afinidad)** | Favorecer personas similares a ti | Comfort con lo familiar, validaci√≥n social | Homogeneidad en contrataci√≥n, groupthink | Structured interviews, diverse panels, criterios objetivos |
| **Contrast Effect (Contraste)** | Juzgar por comparaci√≥n reciente, no absoluto | Procesamiento relativo vs absoluto | Candidato mediocre despu√©s de malo parece excelente | Rubric de evaluaci√≥n absoluto |
| **Social Proof (Prueba social)** | Hacer lo que otros hacen | Validaci√≥n social, miedo a perderse oportunidades (FOMO) | Adoptar tech hype sin evaluar, arquitectura de reba√±o | Evaluate technical fit, no popularity; proof of concepts |
| **Framing Effect (Enmarcado)** | Decisi√≥n cambia seg√∫n c√≥mo se presente | Procesamiento emocional de informaci√≥n | "90% sobrevive" vs "10% muere" generan decisiones diferentes | Present both frames, analizar desde m√∫ltiples perspectivas |
| **Bandwagon Effect (Arrastre)** | "Todos lo hacen, debe ser correcto" | Presi√≥n social, validaci√≥n grupal | Adoptar tecnolog√≠as sin evaluar fit | Revisar fit con contexto espec√≠fico, no popularidad |
| **Efecto Espectador** | No actuar cuando hay m√°s gente | Difusi√≥n de responsabilidad | "Alguien m√°s lo reportar√°" ‚Üí bugs no reportados | Asignar responsabilidades expl√≠citas |

---

## üîç Sesgos Perceptuales y de Evaluaci√≥n

| Sesgo | What | Why ocurre | Impacto | C√≥mo Mitigar |
|:------|:-----|:-----------|:--------|:-------------|
| **Dunning-Kruger Effect** | Incompetentes sobrestiman habilidad, expertos subestiman | Falta de metacognici√≥n en novatos, conciencia de complejidad en expertos | Juniors overconfident, seniors con impostor syndrome | Calibraci√≥n con feedback objetivo, rubricas de evaluaci√≥n claras |
| **Observer-Expectancy Effect** | Expectativas influencian observaci√≥n | Profec√≠a autocumplida, procesamiento selectivo | Self-fulfilling prophecies en testing y m√©tricas | Blind testing, double-blind reviews, procesos estandarizados |
| **Outcome Bias (Por resultados)** | Juzgar decisi√≥n por resultado, no por proceso | Atajo mental para evaluar calidad decisi√≥n | Validar mal proceso porque tuvo suerte, castigar buena decisi√≥n con mal resultado | Evaluar decisi√≥n con info disponible en momento, an√°lisis de proceso |
| **Focus Effect (Efecto foco)** | Sobre-ponderar un aspecto, ignorar el resto | Atenci√≥n limitada, saliencia de ciertas m√©tricas | Obsesionarse con una m√©trica, optimizaci√≥n local vs global | Scorecards balanceadas, m√∫ltiples m√©tricas, perspectiva sist√©mica |
| **Apophenia** | Ver patrones donde no existen | Cerebro busca patrones incluso en ruido | Trading en ruido random, ver "se√±al" en chart patterns aleatorios | An√°lisis estad√≠stico riguroso, tests de significancia |
| **Representativeness Bias (Representatividad)** | Juzgar probabilidad por similitud con estereotipo | Atajos mentales basados en prototipos | Ignorar base rates, "Parece fundador exitoso" ‚Üí invertir sin due diligence | Siempre empezar con base rates, no con similitudes |
| **Illusion of Control (Ilusi√≥n de control)** | Sobrestimar capacidad de controlar eventos | Necesidad psicol√≥gica de control | Tomar riesgos excesivos, "Puedo time el mercado" | Reconocer l√≠mites de control, focus en lo controlable |
| **Optimism Bias (Optimismo)** | Sobrestimar probabilidad de eventos positivos | Mecanismo de protecci√≥n psicol√≥gica | Underestimate riesgos, "A nosotros no nos pasar√°" | Reference class forecasting, an√°lisis de riesgos sistem√°tico |
| **Negativity Bias (Negatividad)** | Ponderar m√°s eventos negativos | Mecanismo evolutivo de supervivencia | Evitar riesgos necesarios, un review malo pesa m√°s que 10 buenos | An√°lisis balanceado, m√©tricas objetivas |
| **Denomination Effect** | Menos dispuesto a gastar billetes grandes vs peque√±os | Percepci√≥n psicol√≥gica del valor | Subestimar gastos peque√±os recurrentes, $5/mes parece barato, $60/a√±o parece caro | Analizar en t√©rminos anuales o lifetime value |

### Sesgos Adicionales de Percepci√≥n y Memoria

| Sesgo | What | Ejemplo | Mitigaci√≥n |
|:------|:-----|:--------|:-----------|
| **Sesgo de Atenci√≥n** | Atender selectivamente a ciertos est√≠mulos | Notar solo bugs en framework que no te gusta | Systematic observation, checklists |
| **Sesgo de Distinci√≥n** | Valorar m√°s cuando comparamos opciones lado a lado | Feature parece mejor en comparaci√≥n directa | Evaluar absolutamente tambi√©n |
| **Sesgo Egoc√©ntrico** | Sobrevalorar propio rol en eventos | "El proyecto fue exitoso gracias a m√≠" | Reconocer contribuciones del equipo |
| **Efecto de Primac√≠a** | Recordar m√°s lo primero | Primera impresi√≥n en entrevista domina evaluaci√≥n | Tomar notas, evaluar al final |
| **Efecto de Retrospecci√≥n "Color de Rosa"** | Recordar pasado mejor de lo que fue | "Los viejos tiempos eran mejores" | Consultar registros hist√≥ricos |
| **Sesgo de Punto Ciego** | No ver propios sesgos | "Yo soy objetivo, los dem√°s est√°n sesgados" | Humildad intelectual, feedback externo |
| **Efecto del Lago Wobegon** | Creer que est√°s por encima del promedio | "Soy mejor developer que la media" (todos creen eso) | Calibraci√≥n con m√©tricas objetivas |

---

## ‚öñÔ∏è Falacias L√≥gicas Cr√≠ticas

| Falacia | What | Ejemplo en Tech | Por qu√© es problema | C√≥mo Refutar |
|:--------|:-----|:----------------|:-------------------|:-------------|
| **Straw Man (Hombre de paja)** | Distorsionar argumento del oponente para refutarlo f√°cil | "¬øQuieres microservicios? ¬øQuieres 100 repos imposibles de mantener?" | Evita discusi√≥n real, crea conflictos artificiales | "Estoy proponiendo servicios bounded context, no microservicios extremos" |
| **Ad Hominem** | Atacar a la persona, no al argumento | "No le hagas caso, es junior/no tiene experiencia" | Ignora m√©ritos del argumento | "Evaluemos la idea por sus m√©ritos t√©cnicos" |
| **Appeal to Authority (Verecundiam)** | "X lo dice, debe ser verdad" | "Google usa microservicios, nosotros tambi√©n debemos" | Ignora contexto espec√≠fico | "Context matters, evaluemos fit con nuestras necesidades" |
| **False Dichotomy (Falso dilema)** | Presentar solo 2 opciones cuando hay m√°s | "O usamos Agile o Waterfall" | Limita opciones creativas, fuerza elecciones sub√≥ptimas | "Hay m√∫ltiples enfoques: h√≠bridos, ScrumBan, enfoques adaptativos" |
| **Slippery Slope (Pendiente resbaladiza)** | "Si hacemos A, inevitablemente pasar√° Z" | "Si permitimos TypeScript, pronto usaremos 10 lenguajes" | Paraliza progreso con miedos exagerados | "Podemos establecer governance y criterios para adopci√≥n controlada" |
| **Post Hoc Ergo Propter Hoc** | "Despu√©s de esto, entonces a causa de esto" | "Deploy en viernes ‚Üí incidente, entonces no deployar viernes" | Atribuci√≥n incorrecta de causas, correlaci√≥n ‚â† causaci√≥n | Analizar causas reales con datos |
| **Red Herring (Pista falsa)** | Desviar la atenci√≥n con tema irrelevante | "S√≠, el c√≥digo tiene bugs, pero mira esta nueva feature" | Evita resolver problemas reales | Volver a t√≥pico original |
| **Begging the Question (Petici√≥n de principio)** | Razonamiento circular, conclusi√≥n asumida en premisa | "Este framework es mejor porque es superior" | Falta de evidencia real | Proveer evidencia externa, benchmarks |
| **False Analogy (Falsa analog√≠a)** | Comparaci√≥n enga√±osa entre cosas no comparables | "Construir software es como construir un edificio" | Modelos mentales incorrectos | "Software se re-construye constantemente, edificios no" |
| **Tu Quoque** | "T√∫ tambi√©n lo haces" | "Me dices que no copie c√≥digo, pero t√∫ lo haces" | Evita responsabilidad personal | "Hipocres√≠a no invalida el argumento" |
| **Appeal to Emotion (Pat√©tico)** | Apelar a emociones en lugar de raz√≥n | "Si no hacemos esto, la empresa quebrar√°" | Decide por miedo, no por datos | Evaluar l√≥gica y datos objetivamente |
| **Appeal to Tradition** | "Siempre se ha hecho as√≠" | "Siempre usamos MySQL, no necesitamos evaluar otras DB" | Impide innovaci√≥n y mejora continua | "Las necesidades evolucionan, debemos evaluar seg√∫n requisitos actuales" |
| **Hasty Generalization (Generalizaci√≥n precipitada)** | Generalizaci√≥n apresurada con muestra peque√±a | "Los dos primeros usuarios se quejaron, entonces a todos les disgusta" | Decisiones basadas en data insuficiente | "Necesitamos muestra estad√≠sticamente significativa" |
| **Bandwagon (Carro)** | "Todos lo hacen, debe ser correcto" | Adoptar hype sin evaluar | Seguir modas sin an√°lisis | Revisar fit con negocio espec√≠fico |
| **Carga de la Prueba** | Invertir quien debe probar | "Demuestra que NO funciona" | Evita responsabilidad de probar afirmaciones | "Quien afirma debe probar" |
| **Appeal to Ignorance (Ignorantiam)** | "No se prob√≥ falso ‚Üí es verdadero" | "No hay evidencia contra esta arquitectura ‚Üí debe ser buena" | Carga de prueba invertida | Ausencia de evidencia ‚â† evidencia de ausencia |
| **Envenenar el Pozo** | Descalificar antes de hablar | "Todo lo que dice X es mentira" | Pre-emptive ad hominem | Evaluar cada argumento por m√©rito |
| **Equ√≠voco** | Cambiar significado de t√©rmino en argumento | "Banco" (instituci√≥n vs asiento) en diferentes premisas | Confusi√≥n conceptual | Definiciones consistentes |
| **Non Sequitur** | Conclusi√≥n no sigue de premisas | "Es martes ‚Üí debemos usar Python" | No hay conexi√≥n l√≥gica | Mostrar falta de conexi√≥n |
| **Afirmaci√≥n del Consecuente** | "Si llueve, hay nubes. Hay nubes ‚Üí llueve" | Confundir suficiente con necesario | L√≥gica inv√°lida | Mostrar contraejemplos |
| **Negaci√≥n del Antecedente** | "Si llueve, uso paraguas. No llueve ‚Üí no uso paraguas" | Confundir necesario con suficiente | L√≥gica inv√°lida | Mostrar contraejemplos |

---

## ‚ö†Ô∏è Leyes y Efectos Parad√≥jicos

| Efecto/Ley | What | Por qu√© es parad√≥jico | Ejemplo en Tech | Mitigaci√≥n |
|:-----------|:-----|:---------------------|:----------------|:-----------|
| **Goodhart's Law** | "Cuando una medida se convierte en objetivo, deja de ser buena medida" | Optimizar m√©tricas corrompe su validez | Optimizar lines of code ‚Üí c√≥digo inflado | M√©tricas m√∫ltiples, auditor√≠as, qualitative + quantitative |
| **Campbell's Law** | Similar a Goodhart: indicador social bajo presi√≥n corrompe procesos | Medici√≥n bajo presi√≥n se corrompe | Teaching to the test, gaming de KPIs | Auditor√≠as, m√©tricas dif√≠ciles de gamear |
| **Cobra Effect** | Soluci√≥n al problema empeora el problema | Incentivos mal dise√±ados crean comportamientos no deseados | Pagar por bugs reportados ‚Üí crear bugs ficticios | Dise√±ar incentivos alineados, m√©tricas balanceadas |
| **Streisand Effect** | Intentar suprimir informaci√≥n la hace m√°s popular | Censura atrae atenci√≥n | Ocultar security issues ‚Üí mayor escrutinio cuando se descubren | Transparencia controlada, comunicaci√≥n proactiva |
| **Peter Principle** | "En jerarqu√≠a, cada empleado tiende a ascender hasta su nivel de incompetencia" | Promoci√≥n basada en performance actual, no en habilidades para nuevo rol | Excelente IC ‚Üí mal manager | Dual track (IC y management), evaluar habilidades espec√≠ficas |
| **Iron Law of Bureaucracy** | Burocracia crece independientemente de su utilidad | Procesos se vuelven fines en s√≠ mismos | Process overload en grandes organizaciones | Regular review de procesos, eliminar redundancias |
| **Efecto de Sobrejustificaci√≥n** | Motivaci√≥n externa reduce intr√≠nseca | Recompensas externas matan motivaci√≥n interna | "Pagar por hobby lo vuelve trabajo" | Preservar motivaci√≥n intr√≠nseca |
| **Efecto de Polarizaci√≥n** | Evidencia fortalece creencias opuestas | Backfire effect | Datos que contradicen creencias las refuerzan | Presentar info de forma no amenazante |
| **Profec√≠a Autocumplida (Pigmali√≥n)** | Expectativas influyen resultado | Creencias crean realidad | Esperar que feature falle ‚Üí no promoverla ‚Üí falla | Awareness de expectativas, testing objetivo |

---

## üìä Sesgos en Data Science y ML

| Sesgo | What | Fase del proceso | Impacto | Mitigaci√≥n |
|:------|:-----|:-----------------|:--------|:-----------|
| **Training Data Bias** | Data no representa poblaci√≥n real | Data collection | Modelo discrimina grupos subrepresentados | Audit datasets, fairness metrics, data augmentation |
| **Label Bias** | Labels incorrectos o sesgados | Data labeling | Modelo aprende sesgos humanos | Multiple labelers, blind labeling, consensus protocols |
| **Confirmation in EDA** | Buscar patrones que confirman hip√≥tesis | Exploratory analysis | Overfitting a narrativa preexistente | Pre-register hypotheses, an√°lisis ciego |
| **P-hacking** | Probar hasta encontrar p<0.05 | Statistical testing | False positives, resultados no reproducibles | Bonferroni correction, pre-registration, replication |

### Ejemplo de C√≥digo Anti-Sesgo en ML

```python
# Mitigaci√≥n de sesgos en pipeline de ML

# 1. Survivorship Bias: Incluir usuarios churned
df = load_full_cohort(include_churned=True)

# 2. Base Rate: Siempre reportar m√©tricas por clase
from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))

# 3. Fairness Audits pre-deploy
from aif360 import metrics
fairness_metric = metrics.ClassificationMetric(
    dataset_true, dataset_pred,
    unprivileged_groups=[{'gender': 0}],
    privileged_groups=[{'gender': 1}]
)
print(f"Disparate Impact: {fairness_metric.disparate_impact()}")

# 4. Selection Bias: Muestreo estratificado
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2
)
```

---

## üõ°Ô∏è Estrategias de Mitigaci√≥n

### 1. Pre-Mortem
**What:** Antes de decidir, asumir que fall√≥ y explicar por qu√©.

**When:** Planning de proyectos, decisiones arquitect√≥nicas grandes.

**How:**
1. "Es un a√±o despu√©s, el proyecto fall√≥. ¬øPor qu√©?"
2. Equipo genera razones de fallo
3. Mitigar top 3 razones identificadas

### 2. Red Team / Devil's Advocate
**What:** Asignar persona/equipo para atacar plan.

**When:** Decisiones cr√≠ticas, cambios arquitect√≥nicos.

**How:** Rotar rol, requiere argumentos contra con evidencia, no solo opini√≥n.

### 3. Decision Journal
**What:** Documentar decisiones antes de conocer outcomes.

**How:** Registrar:
- Decisi√≥n tomada
- Razones y contexto
- Expectativas y predicciones
- Alternativas consideradas
- M√©tricas de √©xito

### 4. Checklists Anti-sesgos
**What:** Forzar considerar m√∫ltiples factores antes de decidir.

**How:** Checklist pre-decisi√≥n con preguntas desafiantes (ver abajo).

### 5. Diverse Teams
**What:** Equipos heterog√©neos cuestionan assumptions.

**How:** Diversidad cognitiva, backgrounds, experiencias, perspectivas.

### 6. Base Rates First
**What:** Siempre empezar con probabilidad base.

**How:** "De 100 startups similares, ¬øcu√°ntos tienen √©xito?" antes de evaluar caso espec√≠fico.

### 7. Prospective Hindsight
**What:** Imaginar futuro como si fuera pasado.

**How:** "Es 2026, ¬øc√≥mo llegamos aqu√≠?" m√°s concreto que "¬øQu√© haremos?"

### 8. Blind Reviews
**What:** Evaluaciones sin conocer identidad del autor.

**How:** Anonymous RFCs, code reviews ciegos, evaluaciones de candidatos sin CV.

### 9. Calibration Sessions
**What:** Comparar predicciones con resultados reales.

**How:** Mensual, comparar estimaciones vs real en retrospectivas.

### 10. Multiple Perspectives
**What:** Analizar desde diferentes √°ngulos.

**How:** Framing positivo y negativo, diferentes stakeholders, m√∫ltiples m√©tricas.

---

## üö´ Se√±ales de Alerta: C√≥mo Reconocer Est√°s Sesgado

### Red Flags Cognitivas (Individual)
- ‚ùå Buscar solo evidencia que confirme tu creencia
- ‚ùå Justificar decisi√≥n post-hoc (racionalizando)
- ‚ùå "Todos piensan como yo" (sin verificar)
- ‚ùå Decisi√≥n r√°pida sin considerar alternativas
- ‚ùå Emocional ante cuestionamiento de idea
- ‚ùå "Esto es diferente" (special pleading)
- ‚ùå Ignorar base rates hist√≥ricos
- ‚ùå No documentar predicciones antes de saber resultado

### Red Flags en Equipos
- ‚ùå Groupthink - consenso r√°pido sin debate
- ‚ùå S√≠ndrome del "not invented here"
- ‚ùå Cultura de "siempre lo hicimos as√≠"
- ‚ùå Falta de diversidad cognitiva
- ‚ùå Miedo a experimentar o fallar
- ‚ùå Decisiones basadas en HiPPO (Highest Paid Person's Opinion)

---

## ‚úÖ Checklist Anti-Sesgos

Antes de decisi√≥n importante:

- [ ] **Pre-mortem:** ¬øPor qu√© podr√≠a fallar esta decisi√≥n?
- [ ] **Devil's advocate:** ¬øQu√© argumentos hay en contra?
- [ ] **Base rate:** ¬øCu√°l es probabilidad hist√≥rica de √©xito en casos similares?
- [ ] **Alternativas:** ¬øConsider√© m√≠nimo 3 opciones diferentes?
- [ ] **Diverse input:** ¬øConsult√© perspectivas de diferentes roles/experiencias?
- [ ] **Decision journal:** ¬øDocument√© razones, expectativas y m√©tricas de √©xito?
- [ ] **Reversibilidad:** ¬øEs reversible? ¬øPuedo testear peque√±o primero?
- [ ] **Data vs intuition:** ¬øTengo datos objetivos o solo "gut feeling"?
- [ ] **Incentivos:** ¬øQu√© comportamientos podr√≠a incentivar esta decisi√≥n (expl√≠cita e impl√≠citamente)?
- [ ] **Future regret:** ¬øDe qu√© me podr√≠a arrepentirme en 6 meses?
- [ ] **Framing:** ¬øAnalic√© desde perspectivas positiva y negativa?
- [ ] **Sunk cost:** ¬øEstoy decidiendo por valor futuro o por inversi√≥n pasada?

---

## üéØ Template para RFCs Anti-Sesgo

```markdown
## [RFC-XXX] T√≠tulo de la Decisi√≥n T√©cnica

### Contexto
[Descripci√≥n del problema y contexto]

### Hip√≥tesis
[Qu√© creemos que mejorar√° y por qu√©]

### Base Rates
[Datos hist√≥ricos relevantes de decisiones similares]

### Alternativas Consideradas
1. **Opci√≥n A:** [Pros/Cons]
2. **Opci√≥n B:** [Pros/Cons]
3. **Opci√≥n C:** [Pros/Cons]

### Decisi√≥n Propuesta
[Opci√≥n elegida y justificaci√≥n]

### Pre-mortem
[¬øPor qu√© podr√≠a fallar esta decisi√≥n?]
1. Riesgo 1 y mitigaci√≥n
2. Riesgo 2 y mitigaci√≥n
3. Riesgo 3 y mitigaci√≥n

### M√©tricas de √âxito
[C√≥mo mediremos si funcion√≥]
- M√©trica 1: [valor actual] ‚Üí [valor objetivo]
- M√©trica 2: [valor actual] ‚Üí [valor objetivo]

### Criterios de Reversi√≥n
[En qu√© condiciones revertiremos la decisi√≥n]

### Reversibilidad
- [ ] Reversible (Type 2 decision)
- [ ] Irreversible (Type 1 decision)

### Timeline
[Cu√°ndo implementar, cu√°ndo evaluar]
```

---

## üéØ Casos Pr√°cticos en Tecnolog√≠a

### Caso 1: Migraci√≥n de Monolito a Microservicios

**Sesgos detectados:**
- Optimismo en estimaciones (Planning Fallacy)
- Anclaje a arquitectura anterior
- Aversi√≥n a p√©rdida de control centralizado
- Social proof ("todos usan microservicios")

**Estrategia anti-sesgo aplicada:**

```python
# Enfoque basado en datos, no opiniones
def migration_strategy():
    # 1. Medir m√©tricas base
    baseline = measure_monolith_metrics()
    
    # 2. Pre-mortem documentado
    risks = document_failure_scenarios()
    
    # 3. Piloto con l√≠mites claros
    pilot = run_pilot(
        service="payment_processing",
        max_duration="6 weeks",
        rollback_criteria=[
            "latency_increase > 20%",
            "error_rate > 1%"
        ]
    )
    
    # 4. Decisi√≥n basada en datos
    return decide_based_on(pilot.metrics, baseline, risks)
```

### Caso 2: Selecci√≥n de Framework para IA

**Sesgos evitados:**
- Efecto halo por hype (ej: "todos usan PyTorch")
- Autoridad (opini√≥n de un senior sin pruebas)
- Falso consenso ("todos en el equipo est√°n de acuerdo")

**Proceso seguido:**
1. Definir requisitos t√©cnicos espec√≠ficos (latencia, escalabilidad, habilidades del equipo)
2. Ejecutar benchmarks objetivos con datos reales
3. Calcular TCO (Total Cost of Ownership) incluyendo mantenimiento
4. Documentar decisi√≥n en RFC con pre-mortem incluido

### Caso 3: Priorizaci√≥n de Features con RICE

**Sesgos mitigados:**
- Voz del cliente ruidosa (un cliente enterprise pide feature compleja)
- Sunk cost en features
- Efecto de encuadre

**Framework aplicado:**

```python
def rice_score(feature):
    """
    RICE = (Reach √ó Impact √ó Confidence) / Effort
    """
    reach = estimate_users_affected()  # Usuarios impactados
    impact = estimate_impact_per_user()  # 0.25, 0.5, 1, 2, 3
    confidence = estimate_confidence()  # 0-100%
    effort = estimate_person_months()
    
    return (reach * impact * confidence) / effort
```

---

## üìö Recursos

### Libros Fundamentales
- [Thinking, Fast and Slow - Daniel Kahneman](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555) - Fundamentos de sesgos cognitivos
- [Predictably Irrational - Dan Ariely](https://www.amazon.com/Predictably-Irrational-Revised-Expanded-Decisions/dp/0061353248) - Econom√≠a del comportamiento
- [The Art of Thinking Clearly - Rolf Dobelli](https://www.amazon.com/Art-Thinking-Clearly-Rolf-Dobelli/dp/0062219693) - 99 sesgos explicados
- [Thinking in Bets - Annie Duke](https://www.amazon.com/Thinking-Bets-Making-Smarter-Decisions/dp/0735216355) - Tomar decisiones bajo incertidumbre
- [The Psychology of Computer Programming - Gerald Weinberg](https://www.amazon.com/Psychology-Computer-Programming-Silver-Anniversary/dp/0932633420) - Sesgos en desarrollo de software
- [Weapons of Math Destruction - Cathy O'Neil](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815) - √âtica en algoritmos

### Recursos Online
- [Wikipedia: List of Cognitive Biases](https://es.wikipedia.org/wiki/Anexo:Sesgos_cognitivos)
- [Farnam Street - Mental Models](https://fs.blog/mental-models/)
- [ML Checklist by Google](https://ml-ops.org)
- [Awesome Cognitive Bias](https://github.com/bugout-dev/awesome-cognitive-biases)
- [Decision Journal Template](https://github.com/davidklaing/decision-journal)

---

## üí° Clave para Semi-Seniors

Reconocer sesgos y falacias no es teor√≠a abstracta: **es lo que evita bugs estrat√©gicos en producto, arquitectura y negocio.**

Los mejores tech leads y PMs no son los que no tienen sesgos (todos los tenemos), sino los que:
1. **Reconocen** sus propios sesgos
2. **Implementan procesos** para mitigarlos
3. **Crean cultura** donde cuestionar assumptions es valorado
4. **Deciden con datos** sobre intuici√≥n cuando es posible
5. **Documentan decisiones** para aprender de aciertos y errores

---

[‚¨ÖÔ∏è Anterior: Onboarding](./28-onboarding.md) | [‚¨ÜÔ∏è Volver arriba](#29-sesgos-cognitivos-falacias-y-leyes-a-evitar) | [‚û°Ô∏è Siguiente: Prompts y Agentes de IA](./30-prompts-agentes.md)